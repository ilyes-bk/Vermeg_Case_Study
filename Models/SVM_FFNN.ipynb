{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**SVM**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ABoi16ZQjsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKy-g3tFoI5d"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, precision_score, roc_auc_score, jaccard_score, hamming_loss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to get the level of a label\n",
        "def get_label_level(label):\n",
        "    return label.count('/') + 1\n",
        "\n",
        "# Function to calculate hops between two labels\n",
        "def calculate_hops(label1, label2):\n",
        "    # Find the common ancestor between the labels\n",
        "    ancestors1 = hierarchy.get(label1, [])\n",
        "    ancestors2 = hierarchy.get(label2, [])\n",
        "    common_ancestor_level = -1\n",
        "\n",
        "    # Find the deepest common ancestor\n",
        "    for ancestor in ancestors1:\n",
        "        if ancestor in ancestors2:\n",
        "            common_ancestor_level = get_label_level(ancestor)\n",
        "            break\n",
        "\n",
        "    # If there's no common ancestor, set hops as -1 to indicate this case\n",
        "    if common_ancestor_level == -1:\n",
        "        return -1\n",
        "\n",
        "    # Calculate the hops by subtracting levels\n",
        "    hops = get_label_level(label1) + get_label_level(label2) - 2 * common_ancestor_level\n",
        "    return hops\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv('/content/selected_features_bugs_final (1).csv')\n",
        "\n",
        "# Step 2: Preprocess the labels\n",
        "df['Label'] = df['Label'].apply(lambda x: x.split('@'))\n",
        "mlb = MultiLabelBinarizer()\n",
        "Y = mlb.fit_transform(df['Label'])\n",
        "\n",
        "# The features (assuming the features are in the other columns)\n",
        "X = df.drop(columns=['Label'])\n",
        "\n",
        "# Step 3: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 4: Train an SVM for each label\n",
        "svm_classifiers = []\n",
        "Y_pred_all = []\n",
        "Y_prob_all = []\n",
        "Y_test_all = []\n",
        "\n",
        "for i, label in enumerate(mlb.classes_):\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y[:, i], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Check if both classes are present in the training set\n",
        "    if len(np.unique(Y_train)) < 2:\n",
        "        print(f\"Skipping label '{label}' due to insufficient classes in training set.\")\n",
        "        continue\n",
        "\n",
        "    # Initialize SVM with probability estimates\n",
        "    svm = SVC(probability=True, class_weight='balanced')\n",
        "\n",
        "    # Train SVM\n",
        "    svm.fit(X_train, Y_train)\n",
        "\n",
        "    # Store the classifier\n",
        "    svm_classifiers.append(svm)\n",
        "\n",
        "    # Predict on the test set\n",
        "    Y_pred = svm.predict(X_test)\n",
        "    Y_prob = svm.predict_proba(X_test)[:, 1]  # Probability for ROC AUC\n",
        "\n",
        "    # Store predictions and true labels for all classifiers\n",
        "    Y_pred_all.append(Y_pred)\n",
        "    Y_prob_all.append(Y_prob)\n",
        "    Y_test_all.append(Y_test)\n",
        "\n",
        "# Stack the predictions and ground truths for all labels\n",
        "Y_pred_all = np.column_stack(Y_pred_all)\n",
        "Y_test_all = np.column_stack(Y_test_all)\n",
        "Y_prob_all = np.column_stack(Y_prob_all)\n",
        "\n",
        "# Convert binary predictions back to label format\n",
        "def binary_to_labels(Y_binary, classes):\n",
        "    label_list = []\n",
        "    for row in Y_binary:\n",
        "        labels = [classes[i] for i, val in enumerate(row) if val == 1]\n",
        "        label_list.append(labels)\n",
        "    return label_list\n",
        "\n",
        "# Convert binary matrices to label format for true and predicted labels\n",
        "true_labels_list = binary_to_labels(Y_test_all, mlb.classes_)\n",
        "pred_labels_list = binary_to_labels(Y_pred_all, mlb.classes_)\n",
        "\n",
        "# Hierarchical Labels\n",
        "hierarchical_labels = [\n",
        "    \"data\",\n",
        "    \"data/structure\",\n",
        "    \"data/structure/column\",\n",
        "    \"data/structure/row\",\n",
        "    \"data/structure/field\",\n",
        "    \"data/database\",\n",
        "    \"data/database/hbase\",\n",
        "    \"data/database/mssql\",\n",
        "    \"data/database/oracle\",\n",
        "    \"data/integrity\",\n",
        "    \"data/integrity/changed\",\n",
        "    \"data/integrity/missing\",\n",
        "    \"data/integrity/wrong\",\n",
        "    \"data/manipulation\",\n",
        "    \"data/manipulation/adjust\",\n",
        "    \"data/manipulation/filter\",\n",
        "    \"data/manipulation/import-export\",\n",
        "    \"data/manipulation/save-delete\",\n",
        "    \"data/manipulation/sort\",\n",
        "    \"data/format\",\n",
        "    \"data/type\",\n",
        "    \"reliability\",\n",
        "    \"reliability/performance\",\n",
        "    \"reliability/performance/latency\",\n",
        "    \"reliability/security\",\n",
        "    \"reliability/code-issues\",\n",
        "    \"reliability/error-handling\",\n",
        "    \"reliability/error-handling/exceptions\",\n",
        "    \"reliability/error-handling/unexpected-errors\",\n",
        "    \"reliability/error-handling/untriggered-errors\",\n",
        "    \"reliability/failures\",\n",
        "    \"reliability/failures/process\",\n",
        "    \"reliability/failures/server\",\n",
        "    \"operation\",\n",
        "    \"operation/fcr\",\n",
        "    \"operation/ignis\",\n",
        "    \"operation/staging\",\n",
        "    \"operation/validation\",\n",
        "    \"operation/designstudio\",\n",
        "    \"operation/designstudio/pipeline\",\n",
        "    \"operation/designstudio/schema\",\n",
        "    \"operation/designstudio/product\",\n",
        "    \"interface\",\n",
        "    \"interface/button\",\n",
        "    \"interface/button/clickbehavior\",\n",
        "    \"interface/button/enable-disable\",\n",
        "    \"interface/display\",\n",
        "    \"interface/display/incorrect\",\n",
        "    \"interface/display/missing\",\n",
        "    \"interface/layout\",\n",
        "    \"interface/layout/box\",\n",
        "    \"interface/layout/grid\",\n",
        "    \"interface/navigation\",\n",
        "    \"interface/navigation/menu\",\n",
        "    \"interface/navigation/search\",\n",
        "]\n",
        "\n",
        "# Create a mapping of labels to their ancestors\n",
        "hierarchy = {}\n",
        "\n",
        "for label in hierarchical_labels:\n",
        "    parts = label.split('/')\n",
        "    # Initialize the list of ancestors for the current label\n",
        "    ancestors = []\n",
        "    # Iterate over parts to get all ancestors\n",
        "    for i in range(len(parts) - 1):  # Exclude the last part (the label itself)\n",
        "        ancestor = '/'.join(parts[:i + 1])  # Create ancestor path\n",
        "        ancestors.append(ancestor)\n",
        "    hierarchy[label] = ancestors\n",
        "\n",
        "# Calculate hierarchical precision and recall\n",
        "def extend_labels(labels):\n",
        "    \"\"\"Extend labels with their ancestors.\"\"\"\n",
        "    extended_labels = set(labels)\n",
        "    for label in labels:\n",
        "        extended_labels.update(hierarchy.get(label, []))\n",
        "    return extended_labels\n",
        "\n",
        "def calculate_hierarchical_metrics(y_true, y_pred):\n",
        "    hP_numerator = 0\n",
        "    hP_denominator = 0\n",
        "    hR_numerator = 0\n",
        "    hR_denominator = 0\n",
        "\n",
        "    for true_labels, pred_labels in zip(y_true, y_pred):\n",
        "        extended_true = extend_labels(true_labels)\n",
        "        extended_pred = extend_labels(pred_labels)\n",
        "\n",
        "        # Hierarchical Precision\n",
        "        intersection = len(extended_true.intersection(extended_pred))\n",
        "        hP_numerator += intersection\n",
        "        hP_denominator += len(extended_pred)\n",
        "\n",
        "        # Hierarchical Recall\n",
        "        hR_numerator += intersection\n",
        "        hR_denominator += len(extended_true)\n",
        "\n",
        "    hP = hP_numerator / hP_denominator if hP_denominator > 0 else 0\n",
        "    hR = hR_numerator / hR_denominator if hR_denominator > 0 else 0\n",
        "\n",
        "    return hP, hR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Calculate hierarchical F1 Score based on hP and hR\n",
        "def calculate_hierarchical_f1(hP, hR):\n",
        "    return (2 * hP * hR) / (hP + hR) if (hP + hR) > 0 else 0\n",
        "\n",
        "# Calculate Hierarchical Precision, Recall, and F1-score\n",
        "hP, hR = calculate_hierarchical_metrics(true_labels_list, pred_labels_list)\n",
        "hF1 = calculate_hierarchical_f1(hP, hR)\n",
        "\n",
        "print(f\"Hierarchical Precision (hP): {hP:.4f}\")\n",
        "print(f\"Hierarchical Recall (hR): {hR:.4f}\")\n",
        "print(f\"Hierarchical F1 Score (hF1): {hF1:.4f}\")\n",
        "\n",
        "# Standard (Flat) Metrics\n",
        "# Flatten the true and predicted binary label arrays for micro/macro calculations\n",
        "Y_test_flat = Y_test_all.ravel()\n",
        "Y_pred_flat = Y_pred_all.ravel()\n",
        "Y_prob_flat = Y_prob_all.ravel()\n",
        "\n",
        "# Micro and Macro Precision, Recall, F1, and Jaccard Score\n",
        "micro_f1 = f1_score(Y_test_all, Y_pred_all, average='micro')\n",
        "macro_f1 = f1_score(Y_test_all, Y_pred_all, average='macro')\n",
        "micro_precision = precision_score(Y_test_all, Y_pred_all, average='micro')\n",
        "macro_precision = precision_score(Y_test_all, Y_pred_all, average='macro')\n",
        "micro_recall = precision_score(Y_test_all, Y_pred_all, average='micro')\n",
        "macro_recall = precision_score(Y_test_all, Y_pred_all, average='macro')\n",
        "micro_jaccard = jaccard_score(Y_test_all, Y_pred_all, average='micro')\n",
        "macro_jaccard = jaccard_score(Y_test_all, Y_pred_all, average='macro')\n",
        "\n",
        "# AUPRC and ROCAUC (only applicable for binary or multilabel case)\n",
        "try:\n",
        "    micro_auprc = average_precision_score(Y_test_all, Y_prob_all, average='micro')\n",
        "    micro_rocauc = roc_auc_score(Y_test_all, Y_prob_all, average='micro')\n",
        "except ValueError as e:\n",
        "    print(\"Error calculating AUPRC/ROCAUC:\", e)\n",
        "    micro_auprc = None\n",
        "    micro_rocauc = None\n",
        "\n",
        "# Hamming Loss\n",
        "hamming_loss_value = hamming_loss(Y_test_all, Y_pred_all)\n",
        "\n",
        "# Display Results\n",
        "print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
        "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
        "print(f\"Micro Precision: {micro_precision:.4f}\")\n",
        "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"Micro Recall: {micro_recall:.4f}\")\n",
        "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
        "print(f\"Micro Jaccard Score: {micro_jaccard:.4f}\")\n",
        "print(f\"Macro Jaccard Score: {macro_jaccard:.4f}\")\n",
        "if micro_auprc is not None:\n",
        "    print(f\"Micro AUPRC: {micro_auprc:.4f}\")\n",
        "if micro_rocauc is not None:\n",
        "    print(f\"Micro ROCAUC: {micro_rocauc:.4f}\")\n",
        "print(f\"Hamming Loss: {hamming_loss_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "3YzdZ7yHeItG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate hierarchical metrics for each instance\n",
        "def calculate_hierarchical_metrics_instance(true_labels, pred_labels):\n",
        "    extended_true = extend_labels(true_labels)\n",
        "    extended_pred = extend_labels(pred_labels)\n",
        "\n",
        "    # Calculate the intersection\n",
        "    intersection = len(extended_true.intersection(extended_pred))\n",
        "\n",
        "    # Calculate hierarchical precision and recall for this instance\n",
        "    hP_instance = intersection / len(extended_pred) if len(extended_pred) > 0 else 0\n",
        "    hR_instance = intersection / len(extended_true) if len(extended_true) > 0 else 0\n",
        "    return hP_instance, hR_instance\n",
        "\n",
        "# Macro-Averaged Hierarchical Precision, Recall, and F1-score\n",
        "macro_hP = 0\n",
        "macro_hR = 0\n",
        "num_instances = len(true_labels_list)\n",
        "\n",
        "for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
        "    hP_instance, hR_instance = calculate_hierarchical_metrics_instance(true_labels, pred_labels)\n",
        "    macro_hP += hP_instance\n",
        "    macro_hR += hR_instance\n",
        "\n",
        "# Calculate macro-averaged precision and recall\n",
        "macro_hP /= num_instances\n",
        "macro_hR /= num_instances\n",
        "macro_hF1 = calculate_hierarchical_f1(macro_hP, macro_hR)\n",
        "\n",
        "# Micro-Averaged Hierarchical Precision, Recall, and F1-score (as previously implemented)\n",
        "hP, hR = calculate_hierarchical_metrics(true_labels_list, pred_labels_list)\n",
        "hF1 = calculate_hierarchical_f1(hP, hR)\n",
        "\n",
        "# Display Macro and Micro Hierarchical Metrics\n",
        "print(f\"Macro-Averaged Hierarchical Precision (hP): {macro_hP:.4f}\")\n",
        "print(f\"Macro-Averaged Hierarchical Recall (hR): {macro_hR:.4f}\")\n",
        "print(f\"Macro-Averaged Hierarchical F1 Score (hF1): {macro_hF1:.4f}\")\n",
        "\n",
        "print(f\"Micro-Averaged Hierarchical Precision (hP): {hP:.4f}\")\n",
        "print(f\"Micro-Averaged Hierarchical Recall (hR): {hR:.4f}\")\n",
        "print(f\"Micro-Averaged Hierarchical F1 Score (hF1): {hF1:.4f}\")\n"
      ],
      "metadata": {
        "id": "cBkp_QrIeXm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize dictionaries to store F1 scores by level\n",
        "f1_macro_by_level = {}\n",
        "f1_micro_by_level = {}\n",
        "\n",
        "for level in range(1, 4):  # Levels 1 to 3, excluding the root\n",
        "    # Get labels belonging to this level\n",
        "    level_labels = [label for label in mlb.classes_ if get_label_level(label) == level]\n",
        "    level_indices = [i for i, label in enumerate(mlb.classes_) if label in level_labels]\n",
        "\n",
        "    # Extract predictions and ground truth for the current level\n",
        "    Y_pred_level = Y_pred_all[:, level_indices]\n",
        "    Y_true_level = Y_test_all[:, level_indices]\n",
        "\n",
        "    # Calculate F1 macro for this level\n",
        "    f1_macro_by_level[level] = f1_score(Y_true_level, Y_pred_level, average='macro', zero_division=0)\n",
        "\n",
        "    # Calculate F1 micro for this level\n",
        "    f1_micro_by_level[level] = f1_score(Y_true_level, Y_pred_level, average='micro', zero_division=0)\n",
        "\n",
        "# Print results for F1 macro and micro by level\n",
        "print(\"F1 Scores by Level:\")\n",
        "for level in range(1, 4):\n",
        "    print(f\"  Level {level}: Macro F1 = {f1_macro_by_level[level]:.4f}, Micro F1 = {f1_micro_by_level[level]:.4f}\")\n"
      ],
      "metadata": {
        "id": "ke0_e6QeVKfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Define the hierarchical graph\n",
        "G = nx.DiGraph()\n",
        "G.add_node(\"root\")\n",
        "\n",
        "# Construct the hierarchy graph with parent-child edges\n",
        "for label in hierarchical_labels:\n",
        "    parts = label.split(\"/\")\n",
        "    if len(parts) == 1:\n",
        "        # Direct child of root\n",
        "        G.add_edge(\"root\", label)\n",
        "    else:\n",
        "        parent = \"/\".join(parts[:-1])\n",
        "        G.add_edge(parent, label)\n",
        "\n",
        "# Function to get the level of a label\n",
        "def get_label_level(label):\n",
        "    return label.count('/') + 1\n",
        "\n",
        "# Function to calculate hops between two labels\n",
        "def calculate_hops(label1, label2):\n",
        "    try:\n",
        "        # Calculate shortest path length between two nodes\n",
        "        return nx.shortest_path_length(G.to_undirected(), source=label1, target=label2)\n",
        "    except nx.NetworkXNoPath:\n",
        "        # Return NaN if no path exists\n",
        "        return np.nan\n",
        "\n",
        "# Function to extend labels with their ancestors in the graph\n",
        "def extend_labels(labels):\n",
        "    extended_labels = set(labels)\n",
        "    for label in labels:\n",
        "        extended_labels.update(nx.ancestors(G, label))\n",
        "    return extended_labels\n",
        "\n",
        "# Calculate hops per instance for level 3 nodes\n",
        "hops_per_instance = []\n",
        "\n",
        "for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
        "    true_level3 = [lbl for lbl in true_labels if get_label_level(lbl) == 3]\n",
        "    pred_level3 = [lbl for lbl in pred_labels if get_label_level(lbl) == 3]\n",
        "\n",
        "    # Calculate hops between all TP and FP level 3 pairs\n",
        "    hops = [calculate_hops(tp, fp) for tp in true_level3 for fp in pred_level3 if tp != fp]\n",
        "    hops = [h for h in hops if not np.isnan(h)]  # Filter out NaN values\n",
        "\n",
        "    if hops:\n",
        "        # Store the mean hops for each instance\n",
        "        hops_per_instance.append(np.mean(hops))\n",
        "\n",
        "# Calculate mean hops and margin of error\n",
        "mean_hops = np.mean(hops_per_instance) if hops_per_instance else np.nan\n",
        "std_dev_hops = np.std(hops_per_instance) if hops_per_instance else np.nan\n",
        "margin_of_error = 1.96 * std_dev_hops / np.sqrt(len(hops_per_instance)) if hops_per_instance else np.nan\n",
        "\n",
        "print(f\"Mean hops: {mean_hops:.2f}, Margin of error: ±{margin_of_error:.2f}\")\n"
      ],
      "metadata": {
        "id": "aP4T9e5ipMQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **FFNN**"
      ],
      "metadata": {
        "id": "8UFrFWFRPszO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, hamming_loss, f1_score, jaccard_score, average_precision_score, precision_recall_curve, roc_auc_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Your CSV file name from the uploaded files\n",
        "csv_file = \"/content/selected_features_bugs_final (1).csv\"\n",
        "\n",
        "# Load CSV data\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Extract features and labels\n",
        "X = data.drop(columns=['Label']).values\n",
        "y = data['Label'].apply(lambda x: x.split('@')).values\n",
        "\n",
        "# Binarize multi-labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_binarized = mlb.fit_transform(y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binarized, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing: Imputation and scaling\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "imputer = SimpleImputer(strategy='mean').fit(X_train)\n",
        "X_train = torch.tensor(scaler.transform(imputer.transform(X_train)), dtype=torch.float32)\n",
        "X_test = torch.tensor(scaler.transform(imputer.transform(X_test)), dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Define dataset and data loaders\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Define the model\n",
        "class FlatFFNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, hyperparams):\n",
        "        super(FlatFFNNModel, self).__init__()\n",
        "        self.nb_layers = hyperparams['num_layers']\n",
        "        fc = []\n",
        "        for i in range(self.nb_layers):\n",
        "            if i == 0:\n",
        "                fc.append(nn.Linear(input_dim, hidden_dim))\n",
        "            elif i == self.nb_layers - 1:\n",
        "                fc.append(nn.Linear(hidden_dim, output_dim))\n",
        "            else:\n",
        "                fc.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        self.fc = nn.ModuleList(fc)\n",
        "        self.drop = nn.Dropout(hyperparams['dropout'])\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.f = nn.ReLU() if hyperparams['non_lin'] == 'relu' else nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.nb_layers):\n",
        "            if i == self.nb_layers - 1:\n",
        "                x = self.sigmoid(self.fc[i](x))\n",
        "            else:\n",
        "                x = self.f(self.fc[i](x))\n",
        "                x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "# Define model hyperparameters\n",
        "hyperparams = {'batch_size': 4, 'num_layers': 3, 'dropout': 0.7, 'non_lin': 'relu', 'hidden_dim': 1000, 'lr': 1e-5, 'weight_decay': 1e-5}\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = FlatFFNNModel(input_dim, hyperparams['hidden_dim'], output_dim, hyperparams).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'], weight_decay=hyperparams['weight_decay'])\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):  # Adjust number of epochs as needed\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/100], Loss: {epoch_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "y_pred, y_prob = [], []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        features = features.to(device)\n",
        "        outputs = model(features)\n",
        "        y_pred.append((outputs > 0.5).cpu())\n",
        "        y_prob.append(outputs.cpu())\n",
        "\n",
        "y_pred = torch.cat(y_pred).numpy()\n",
        "y_prob = torch.cat(y_prob).numpy()\n",
        "y_test = y_test.numpy()\n",
        "\n",
        "# Evaluation metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test, y_pred),\n",
        "    \"Hamming Loss\": hamming_loss(y_test, y_pred),\n",
        "    \"F1 Score (micro)\": f1_score(y_test, y_pred, average='micro'),\n",
        "    \"Jaccard Score (samples)\": jaccard_score(y_test, y_pred, average='samples'),\n",
        "    \"Average Precision\": average_precision_score(y_test, y_prob, average='micro'),\n",
        "    \"ROC AUC Score\": roc_auc_score(y_test, y_prob, average='micro'),\n",
        "}\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "precision, recall, _ = precision_recall_curve(y_test.ravel(), y_prob.ravel())\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall, precision, label='Precision-Recall Curve', linestyle='--', marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Hierarchical metrics implementation ---\n",
        "\n",
        "# Define the hierarchical labels\n",
        "hierarchical_labels = [\n",
        "    \"data\",\n",
        "    \"data/structure\",\n",
        "    \"data/structure/column\",\n",
        "    \"data/structure/row\",\n",
        "    \"data/structure/field\",\n",
        "    \"data/database\",\n",
        "    \"data/database/hbase\",\n",
        "    \"data/database/mssql\",\n",
        "    \"data/database/oracle\",\n",
        "    \"data/integrity\",\n",
        "    \"data/integrity/changed\",\n",
        "    \"data/integrity/missing\",\n",
        "    \"data/integrity/wrong\",\n",
        "    \"data/manipulation\",\n",
        "    \"data/manipulation/adjust\",\n",
        "    \"data/manipulation/filter\",\n",
        "    \"data/manipulation/import-export\",\n",
        "    \"data/manipulation/save-delete\",\n",
        "    \"data/manipulation/sort\",\n",
        "    \"data/format\",\n",
        "    \"data/type\",\n",
        "    \"reliability\",\n",
        "    \"reliability/performance\",\n",
        "    \"reliability/performance/latency\",\n",
        "    \"reliability/security\",\n",
        "    \"reliability/code-issues\",\n",
        "    \"reliability/error-handling\",\n",
        "    \"reliability/error-handling/exceptions\",\n",
        "    \"reliability/error-handling/unexpected-errors\",\n",
        "    \"reliability/error-handling/untriggered-errors\",\n",
        "    \"reliability/failures\",\n",
        "    \"reliability/failures/process\",\n",
        "    \"reliability/failures/server\",\n",
        "    \"operation\",\n",
        "    \"operation/fcr\",\n",
        "    \"operation/ignis\",\n",
        "    \"operation/staging\",\n",
        "    \"operation/validation\",\n",
        "    \"operation/designstudio\",\n",
        "    \"operation/designstudio/pipeline\",\n",
        "    \"operation/designstudio/schema\",\n",
        "    \"operation/designstudio/product\",\n",
        "    \"interface\",\n",
        "    \"interface/button\",\n",
        "    \"interface/button/clickbehavior\",\n",
        "    \"interface/button/enable-disable\",\n",
        "    \"interface/display\",\n",
        "    \"interface/display/incorrect\",\n",
        "    \"interface/display/missing\",\n",
        "    \"interface/layout\",\n",
        "    \"interface/layout/box\",\n",
        "    \"interface/layout/grid\",\n",
        "    \"interface/navigation\",\n",
        "    \"interface/navigation/menu\",\n",
        "    \"interface/navigation/search\",\n",
        "]\n",
        "\n",
        "\n",
        "# Define hierarchy mapping for hierarchical metrics and hops calculation\n",
        "hierarchy = {}\n",
        "for label in hierarchical_labels:\n",
        "    parts = label.split('/')\n",
        "    ancestors = []\n",
        "    for i in range(len(parts) - 1):\n",
        "        ancestor = '/'.join(parts[:i + 1])\n",
        "        ancestors.append(ancestor)\n",
        "    hierarchy[label] = ancestors\n",
        "\n",
        "# Helper function to determine label level based on the number of slashes\n",
        "def get_label_level(label):\n",
        "    return label.count('/') + 1\n",
        "\n",
        "# Extend labels to include their ancestors for hierarchical metrics\n",
        "def extend_labels(labels):\n",
        "    extended_labels = set(labels)\n",
        "    for label in labels:\n",
        "        extended_labels.update(hierarchy.get(label, []))\n",
        "    return extended_labels\n",
        "\n",
        "# Convert binary matrices to label lists\n",
        "def binary_to_labels(Y_binary, classes):\n",
        "    label_list = []\n",
        "    for row in Y_binary:\n",
        "        labels = [classes[i] for i, val in enumerate(row) if val == 1]\n",
        "        label_list.append(labels)\n",
        "    return label_list\n",
        "\n",
        "# Calculate hierarchical precision, recall, and F-measure\n",
        "def calculate_hierarchical_metrics(y_true, y_pred):\n",
        "    hP_numerator = 0\n",
        "    hP_denominator = 0\n",
        "    hR_numerator = 0\n",
        "    hR_denominator = 0\n",
        "\n",
        "    for true_labels, pred_labels in zip(y_true, y_pred):\n",
        "        extended_true = extend_labels(true_labels)\n",
        "        extended_pred = extend_labels(pred_labels)\n",
        "\n",
        "        intersection = len(extended_true.intersection(extended_pred))\n",
        "        hP_numerator += intersection\n",
        "        hP_denominator += len(extended_pred) or 1\n",
        "        hR_numerator += intersection\n",
        "        hR_denominator += len(extended_true) or 1\n",
        "\n",
        "    hP = hP_numerator / hP_denominator\n",
        "    hR = hR_numerator / hR_denominator\n",
        "    return hP, hR\n",
        "\n",
        "def calculate_hF(hP, hR, beta=1):\n",
        "    if hP + hR == 0:\n",
        "        return 0\n",
        "    return ( (beta ** 2 + 1) * hP * hR ) / ( beta ** 2 * hP + hR )\n",
        "\n",
        "\n",
        "\n",
        "hP, hR = calculate_hierarchical_metrics(true_labels_list, pred_labels_list)\n",
        "hF = calculate_hF(hP, hR)\n",
        "\n",
        "# Output the hierarchical results\n",
        "print(f\"Hierarchical Precision (hP): {hP:.4f}\")\n",
        "print(f\"Hierarchical Recall (hR): {hR:.4f}\")\n",
        "print(f\"Hierarchical F-measure (hF): {hF:.4f}\")\n"
      ],
      "metadata": {
        "id": "TAuO3vYGv_DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "# Define the hierarchical graph\n",
        "G = nx.DiGraph()\n",
        "G.add_node(\"root\")\n",
        "\n",
        "# Construct the hierarchy graph with parent-child edges\n",
        "for label in hierarchical_labels:\n",
        "    parts = label.split(\"/\")\n",
        "    if len(parts) == 1:\n",
        "        # Direct child of root\n",
        "        G.add_edge(\"root\", label)\n",
        "    else:\n",
        "        parent = \"/\".join(parts[:-1])\n",
        "        G.add_edge(parent, label)\n",
        "\n",
        "# Function to get the level of a label\n",
        "def get_label_level(label):\n",
        "    return label.count('/') + 1\n",
        "\n",
        "# Function to calculate hops between two labels\n",
        "def calculate_hops(label1, label2):\n",
        "    try:\n",
        "        # Calculate shortest path length between two nodes\n",
        "        return nx.shortest_path_length(G.to_undirected(), source=label1, target=label2)\n",
        "    except nx.NetworkXNoPath:\n",
        "        # Return NaN if no path exists\n",
        "        return np.nan\n",
        "\n",
        "# Function to extend labels with their ancestors in the graph\n",
        "def extend_labels(labels):\n",
        "    extended_labels = set(labels)\n",
        "    for label in labels:\n",
        "        extended_labels.update(nx.ancestors(G, label))\n",
        "    return extended_labels\n",
        "\n",
        "# Calculate hops per instance for level 3 nodes\n",
        "hops_per_instance = []\n",
        "\n",
        "for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
        "    true_level3 = [lbl for lbl in true_labels if get_label_level(lbl) == 3]\n",
        "    pred_level3 = [lbl for lbl in pred_labels if get_label_level(lbl) == 3]\n",
        "\n",
        "    # Calculate hops between all TP and FP level 3 pairs\n",
        "    hops = [calculate_hops(tp, fp) for tp in true_level3 for fp in pred_level3 if tp != fp]\n",
        "    hops = [h for h in hops if not np.isnan(h)]  # Filter out NaN values\n",
        "\n",
        "    if hops:\n",
        "        # Store the mean hops for each instance\n",
        "        hops_per_instance.append(np.mean(hops))\n",
        "\n",
        "# Calculate mean hops and margin of error\n",
        "mean_hops = np.mean(hops_per_instance) if hops_per_instance else np.nan\n",
        "std_dev_hops = np.std(hops_per_instance) if hops_per_instance else np.nan\n",
        "margin_of_error = 1.96 * std_dev_hops / np.sqrt(len(hops_per_instance)) if hops_per_instance else np.nan\n",
        "\n",
        "print(f\"Mean hops: {mean_hops:.2f}, Margin of error: ±{margin_of_error:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQQa3xeNYP6c",
        "outputId": "e4cc41d0-bfe7-4f4f-fcad-9675dc2d435d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean hops: 5.27, Margin of error: ±0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from collections import defaultdict\n",
        "\n",
        "# Helper to get labels by level\n",
        "def get_labels_by_level(y_binary, classes, level):\n",
        "    return [[label for label in binary_to_labels([row], classes)[0] if get_label_level(label) == level] for row in y_binary]\n",
        "\n",
        "\n",
        "\n",
        "# F1 scores (macro and micro) by level\n",
        "def f1_by_level(y_true, y_pred, classes):\n",
        "    f1_scores = {'macro': {}, 'micro': {}}\n",
        "    for level in range(1, 4):\n",
        "        y_true_level = get_labels_by_level(y_true, classes, level)\n",
        "        y_pred_level = get_labels_by_level(y_pred, classes, level)\n",
        "\n",
        "        # Binarize per level for F1 calculation\n",
        "        mlb = MultiLabelBinarizer(classes=[c for c in classes if get_label_level(c) == level])\n",
        "        y_true_bin = mlb.fit_transform(y_true_level)\n",
        "        y_pred_bin = mlb.transform(y_pred_level)\n",
        "\n",
        "        f1_scores['macro'][level] = f1_score(y_true_bin, y_pred_bin, average='macro')\n",
        "        f1_scores['micro'][level] = f1_score(y_true_bin, y_pred_bin, average='micro')\n",
        "    return f1_scores\n",
        "\n",
        "\n",
        "\n",
        "# Example usage with y_test and y_pred\n",
        "f1_scores_by_level = f1_by_level(y_test, y_pred, mlb.classes_)\n",
        "print(\"F1 Scores by Level:\", f1_scores_by_level)\n",
        "\n"
      ],
      "metadata": {
        "id": "mdbGmE8kpuyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate hierarchical F1 Score based on hP and hR\n",
        "def calculate_hierarchical_f1(hP, hR):\n",
        "    return (2 * hP * hR) / (hP + hR) if (hP + hR) > 0 else 0\n",
        "\n",
        "# Function to calculate hierarchical metrics for each instance\n",
        "def calculate_hierarchical_metrics_instance(true_labels, pred_labels):\n",
        "    extended_true = extend_labels(true_labels)\n",
        "    extended_pred = extend_labels(pred_labels)\n",
        "\n",
        "    # Calculate the intersection\n",
        "    intersection = len(extended_true.intersection(extended_pred))\n",
        "\n",
        "    # Calculate hierarchical precision and recall for this instance\n",
        "    hP_instance = intersection / len(extended_pred) if len(extended_pred) > 0 else 0\n",
        "    hR_instance = intersection / len(extended_true) if len(extended_true) > 0 else 0\n",
        "    return hP_instance, hR_instance\n",
        "\n",
        "# Macro-Averaged Hierarchical Precision, Recall, and F1-score\n",
        "macro_hP = 0\n",
        "macro_hR = 0\n",
        "num_instances = len(true_labels_list)\n",
        "\n",
        "for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
        "    hP_instance, hR_instance = calculate_hierarchical_metrics_instance(true_labels, pred_labels)\n",
        "    macro_hP += hP_instance\n",
        "    macro_hR += hR_instance\n",
        "\n",
        "# Calculate macro-averaged precision and recall\n",
        "macro_hP /= num_instances\n",
        "macro_hR /= num_instances\n",
        "macro_hF1 = calculate_hierarchical_f1(macro_hP, macro_hR)\n",
        "\n",
        "# Micro-Averaged Hierarchical Precision, Recall, and F1-score (as previously implemented)\n",
        "hP, hR = calculate_hierarchical_metrics(true_labels_list, pred_labels_list)\n",
        "hF1 = calculate_hierarchical_f1(hP, hR)\n",
        "\n",
        "# Display Macro and Micro Hierarchical Metrics\n",
        "print(f\"Macro-Averaged Hierarchical Precision (hP): {macro_hP:.4f}\")\n",
        "print(f\"Macro-Averaged Hierarchical Recall (hR): {macro_hR:.4f}\")\n",
        "print(f\"Macro-Averaged Hierarchical F1 Score (hF1): {macro_hF1:.4f}\")\n",
        "\n",
        "print(f\"Micro-Averaged Hierarchical Precision (hP): {hP:.4f}\")\n",
        "print(f\"Micro-Averaged Hierarchical Recall (hR): {hR:.4f}\")\n",
        "print(f\"Micro-Averaged Hierarchical F1 Score (hF1): {hF1:.4f}\")\n"
      ],
      "metadata": {
        "id": "hkJ2UACncAoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Convert binary matrices to label lists for FFNN output\n",
        "true_labels_list = binary_to_labels(y_test, mlb.classes_)\n",
        "pred_labels_list = binary_to_labels(y_pred, mlb.classes_)\n",
        "\n",
        "\n",
        "# Calculate micro and macro precision, recall, and F1 scores\n",
        "micro_precision = precision_score(y_test, y_pred, average='micro')\n",
        "micro_recall = recall_score(y_test, y_pred, average='micro')\n",
        "micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
        "\n",
        "macro_precision = precision_score(y_test, y_pred, average='macro')\n",
        "macro_recall = recall_score(y_test, y_pred, average='macro')\n",
        "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "# Output micro and macro metrics\n",
        "print(\"\\nMicro Metrics:\")\n",
        "print(f\"Precision: {micro_precision:.4f}\")\n",
        "print(f\"Recall: {micro_recall:.4f}\")\n",
        "print(f\"F1 Score: {micro_f1:.4f}\")\n",
        "\n",
        "print(\"\\nMacro Metrics:\")\n",
        "print(f\"Precision: {macro_precision:.4f}\")\n",
        "print(f\"Recall: {macro_recall:.4f}\")\n",
        "print(f\"F1 Score: {macro_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "WQrnWOX4coY0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}